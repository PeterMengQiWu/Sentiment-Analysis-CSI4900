{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ec4a4b1fed8cdcc39317a02ea85456e6a3ae5ccf8fcdd15a3382d60cec052de8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from nltk import pos_tag\r\n",
    "from nltk.corpus import sentiwordnet as swn\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.util import ngrams\r\n",
    "from gensim.corpora import Dictionary\r\n",
    "from gensim.models import Word2Vec, WordEmbeddingSimilarityIndex\r\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\r\n",
    "from gensim.similarities import SparseTermSimilarityMatrix, SparseMatrixSimilarity, SoftCosineSimilarity\r\n",
    "from gensim.matutils import softcossim\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "from collections import Counter\r\n",
    "from statistics import stdev, mean\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import os, json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "socialsent_smoke_filepath = './lexicons/subreddits/electronic_cigarette.tsv'\r\n",
    "socialsent_askscience_filepath = './lexicons/subreddits/askscience.tsv'\r\n",
    "socialsent_science_filepath = './lexicons/subreddits/science.tsv'\r\n",
    "\r\n",
    "socialsent_df = pd.read_csv(socialsent_smoke_filepath, sep='\\t', names=['word', 'avg_sent_score', 'std_sent_score'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def read_files(directory):\r\n",
    "    word2vec_texts = {}\r\n",
    "    doc2vec_texts = {}\r\n",
    "    for dir in os.listdir(directory):\r\n",
    "        word2vec_texts[dir] = []\r\n",
    "        doc2vec_texts[dir] = []\r\n",
    "        for root, _, files in os.walk(os.path.join(directory, dir)):\r\n",
    "            for file in files:\r\n",
    "                open_file = open(os.path.join(root, file), 'r')\r\n",
    "                text = open_file.read()\r\n",
    "                open_file.close()\r\n",
    "                word2vec_texts[dir] += [word_tokenize(sentence) for sentence in text.split('\\n') if len(word_tokenize(sentence)) > 0]\r\n",
    "                doc2vec_texts[dir].append(text.replace('\\n', ' '))\r\n",
    "    return word2vec_texts, doc2vec_texts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def read_raw_files(directory):\r\n",
    "    raw_texts = {}\r\n",
    "    for dir in os.listdir(directory):\r\n",
    "        raw_texts[dir] = []\r\n",
    "        for root, _, files in os.walk(os.path.join(directory, dir)):\r\n",
    "            for file in files:\r\n",
    "                open_file = open(os.path.join(root, file), 'r')\r\n",
    "                text = open_file.read()\r\n",
    "                open_file.close()\r\n",
    "                raw_texts[dir] += [word_tokenize(sentence) for sentence in text.split('\\n') if len(word_tokenize(sentence)) > 0]\r\n",
    "    return raw_texts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def build_unigram_model(texts):\n",
    "    unigrams = {}\n",
    "    for subset in texts:\n",
    "        unigrams[subset] = list(ngrams([word for sentence in texts[subset] for word in sentence], 1))\n",
    "    return unigrams"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def evaluate_unigram_model(unigrams, tagged_words):\r\n",
    "    common_words = {}\r\n",
    "    common_tagged_words = {}\r\n",
    "    for subset1 in unigrams:\r\n",
    "        common_words[subset1] = {}\r\n",
    "        common_tagged_words[subset1] = {}\r\n",
    "        counter1 = Counter(unigrams[subset1])\r\n",
    "        counter1_tagged = Counter(tagged_words[subset1])\r\n",
    "        subset1_top100 = [word[0][0] for word in counter1.most_common(100)]\r\n",
    "        subset1_top100_tagged = [word[0] for word in counter1_tagged.most_common(100)]\r\n",
    "        for subset2 in unigrams:\r\n",
    "            counter2 = Counter(unigrams[subset2])\r\n",
    "            counter2_tagged = Counter(tagged_words[subset2])\r\n",
    "            subset2_top100 = [word[0][0] for word in counter2.most_common(100)]\r\n",
    "            subset2_top100_tagged = [word[0] for word in counter2_tagged.most_common(100)]\r\n",
    "            common_words[subset1][subset2] = [[word for word in subset1_top100 if word in subset2_top100]]\r\n",
    "            common_tagged_words[subset1][subset2] = [[word for word in subset1_top100_tagged if word in subset2_top100_tagged]]\r\n",
    "    senti_scores = {}\r\n",
    "    for subset in common_tagged_words:\r\n",
    "        senti_scores[subset], _ = sentiwordnet_scores(common_tagged_words[subset], True)\r\n",
    "    return common_words, common_tagged_words, senti_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_wordnet_pos(tag):\r\n",
    "    if tag.startswith('J'):\r\n",
    "        return 'a'\r\n",
    "    elif tag.startswith('V'):\r\n",
    "        return 'v'\r\n",
    "    elif tag.startswith('N'):\r\n",
    "        return 'n'\r\n",
    "    elif tag.startswith('R'):\r\n",
    "        return 'r'\r\n",
    "    else:\r\n",
    "        return ''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_mpqa_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'adj'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'verb'\n",
    "    elif tag.startswith('N'):\n",
    "        return 'noun'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'adverb'\n",
    "    else:\n",
    "        return 'anypos'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def sentiwordnet_scores(texts, tagged=False):\r\n",
    "    senti_scores = {}\r\n",
    "    word_list = {}\r\n",
    "    for subset in texts:\r\n",
    "        senti_scores[subset] = {\r\n",
    "            'total_positive_score': 0,\r\n",
    "            'total_negative_score': 0,\r\n",
    "            'total_objective_score': 0,\r\n",
    "            'average_positive_score': 0,\r\n",
    "            'std_dev_positive_score': 0,\r\n",
    "            'average_negative_score': 0,\r\n",
    "            'std_dev_negative_score': 0,\r\n",
    "            'average_objective_score': 0,\r\n",
    "            'std_dev_objective_score': 0,\r\n",
    "            'overall_score': 0,\r\n",
    "            'total_num_objective_posts': 0,\r\n",
    "            'total_num_words': 0,\r\n",
    "            'positive_count': 0,\r\n",
    "            'negative_count': 0,\r\n",
    "            'objective_count': 0,\r\n",
    "            'positive_ratio': 0,\r\n",
    "            'negative_ratio': 0\r\n",
    "        }\r\n",
    "        pos_scores = []\r\n",
    "        neg_scores = []\r\n",
    "        obj_scores = []\r\n",
    "        word_list[subset] = []\r\n",
    "        for line in texts[subset]:\r\n",
    "            if not tagged:\r\n",
    "                tagged_line = pos_tag(line)\r\n",
    "            else:\r\n",
    "                tagged_line = line\r\n",
    "            for tag in tagged_line:\r\n",
    "                synsets = swn.senti_synsets(tag[0], get_wordnet_pos(tag[1])) if get_wordnet_pos(tag[1]) != '' else []\r\n",
    "                synsets_list = list(synsets)\r\n",
    "                if len(synsets_list) > 0:\r\n",
    "                    word_list[subset].append(tag)\r\n",
    "                    senti_scores[subset]['total_num_words'] += 1\r\n",
    "                    synset = synsets_list[0]\r\n",
    "                    pos_score = synset.pos_score()\r\n",
    "                    neg_score = synset.neg_score()\r\n",
    "                    obj_score = synset.obj_score() \r\n",
    "                    if pos_score == max([pos_score, neg_score, obj_score]): \r\n",
    "                        senti_scores[subset]['positive_count'] += 1\r\n",
    "                        senti_scores[subset]['total_positive_score'] += pos_score\r\n",
    "                        pos_scores.append(pos_score)\r\n",
    "                    elif neg_score == max([pos_score, neg_score, obj_score]):\r\n",
    "                        senti_scores[subset]['negative_count'] += 1\r\n",
    "                        senti_scores[subset]['total_negative_score'] += neg_score\r\n",
    "                        neg_scores.append(neg_score)\r\n",
    "                    elif obj_score == max([pos_score, neg_score, obj_score]):\r\n",
    "                        senti_scores[subset]['objective_count'] += 1\r\n",
    "                        senti_scores[subset]['total_objective_score'] += obj_score\r\n",
    "                        obj_scores.append(obj_score)\r\n",
    "\r\n",
    "        senti_scores[subset]['average_positive_score'] = round(senti_scores[subset]['total_positive_score'] / senti_scores[subset]['total_num_words'], 4)\r\n",
    "        senti_scores[subset]['average_negative_score'] = round(senti_scores[subset]['total_negative_score'] / senti_scores[subset]['total_num_words'], 4)\r\n",
    "        senti_scores[subset]['average_objective_score'] = round(senti_scores[subset]['total_objective_score'] / senti_scores[subset]['total_num_words'], 4)\r\n",
    "        if len(pos_scores) > 2:\r\n",
    "            senti_scores[subset]['std_dev_positive_score'] = round(stdev(pos_scores), 4)\r\n",
    "        else:\r\n",
    "            senti_scores[subset]['std_dev_positive_score'] = 0\r\n",
    "        if len(neg_scores) > 2:\r\n",
    "            senti_scores[subset]['std_dev_negative_score'] = round(stdev(neg_scores), 4)\r\n",
    "        else:\r\n",
    "            senti_scores[subset]['std_dev_negative_score'] = 0\r\n",
    "        if len(obj_scores) > 2:\r\n",
    "            senti_scores[subset]['std_dev_objective_score'] = round(stdev(obj_scores), 4)\r\n",
    "        else:\r\n",
    "            senti_scores[subset]['std_dev_objective_score'] = 0\r\n",
    "        senti_scores[subset]['overall_score'] = senti_scores[subset]['average_negative_score'] - senti_scores[subset]['average_positive_score']\r\n",
    "        senti_scores[subset]['positive_ratio'] = round(senti_scores[subset]['positive_count'] / senti_scores[subset]['total_num_words'], 4)\r\n",
    "        senti_scores[subset]['negative_ratio'] = round(senti_scores[subset]['negative_count'] / senti_scores[subset]['total_num_words'], 4)\r\n",
    "        \r\n",
    "    return senti_scores, word_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def mpqa_scores(texts, tagged=False):\r\n",
    "    mpqa_filepath = './lexicons/mpqa/mpqa.json'\r\n",
    "    with open(mpqa_filepath, 'r') as input:\r\n",
    "        mpqa = json.load(input)\r\n",
    "    mpqa_scores = {}\r\n",
    "    word_list = {}\r\n",
    "    for subset in texts:\r\n",
    "        mpqa_scores[subset] = {\r\n",
    "            'total_positive_score': 0,\r\n",
    "            'total_negative_score': 0,\r\n",
    "            'total_objective_score': 0,\r\n",
    "            'average_positive_score': 0,\r\n",
    "            'std_dev_positive_score': 0,\r\n",
    "            'average_negative_score': 0,\r\n",
    "            'std_dev_negative_score': 0,\r\n",
    "            'average_objective_score': 0,\r\n",
    "            'std_dev_objective_score': 0,\r\n",
    "            'overall_score': 0,\r\n",
    "            'total_num_objective_posts': 0,\r\n",
    "            'total_num_words': 0,\r\n",
    "            'positive_count': 0,\r\n",
    "            'negative_count': 0,\r\n",
    "            'objective_count': 0,\r\n",
    "            'positive_ratio': 0,\r\n",
    "            'negative_ratio': 0\r\n",
    "        }\r\n",
    "        pos_scores = []\r\n",
    "        neg_scores = []\r\n",
    "        obj_scores = []\r\n",
    "        word_list[subset] = []\r\n",
    "        for line in texts[subset]:\r\n",
    "            if not tagged:\r\n",
    "                tagged_line = pos_tag(line)\r\n",
    "            else:\r\n",
    "                tagged_line = line\r\n",
    "            for tag in tagged_line:\r\n",
    "               pos = get_mpqa_pos(tag[1])\r\n",
    "               word = tag[0]\r\n",
    "               if word in mpqa and (pos in mpqa[word] or 'anypos' in mpqa[word]):\r\n",
    "                    word_list[subset].append(tag)\r\n",
    "                    mpqa_scores[subset]['total_num_words'] += 1\r\n",
    "                    pos_score = 0\r\n",
    "                    neg_score = 0\r\n",
    "                    obj_score = 0\r\n",
    "                    if ('anypos' in mpqa[word] and mpqa[word]['anypos'] == 'positive') or (pos in mpqa[word] and mpqa[word][pos] == 'positive'):\r\n",
    "                        pos_score = 1\r\n",
    "                        mpqa_scores[subset]['total_positive_score'] += pos_score\r\n",
    "                        pos_scores.append(pos_score)\r\n",
    "                    elif ('anypos' in mpqa[word] and mpqa[word]['anypos'] == 'negative') or (pos in mpqa[word] and mpqa[word][pos] == 'negative'):\r\n",
    "                        neg_score = 1\r\n",
    "                        mpqa_scores[subset]['total_negative_score'] += neg_score\r\n",
    "                        neg_scores.append(neg_score)\r\n",
    "                    elif ('anypos' in mpqa[word] and mpqa[word]['anypos'] == 'neutral') or (pos in mpqa[word] and mpqa[word][pos] == 'neutral'):\r\n",
    "                        obj_score = 1\r\n",
    "                        mpqa_scores[subset]['total_objective_score'] += obj_score\r\n",
    "                        obj_scores.append(obj_score)\r\n",
    "                    elif ('anypos' in mpqa[word] and mpqa[word]['anypos'] == 'both') or (pos in mpqa[word] and mpqa[word][pos] == 'both'):\r\n",
    "                        pos_score = 1 # always label the word as positive if both sentiment is possible\r\n",
    "                        mpqa_scores[subset]['total_positive_score'] += pos_score\r\n",
    "                        pos_scores.append(pos_score)\r\n",
    "\r\n",
    "                    if pos_score == max([pos_score, neg_score, obj_score]): \r\n",
    "                        mpqa_scores[subset]['positive_count'] += 1\r\n",
    "                    elif neg_score == max([pos_score, neg_score, obj_score]):\r\n",
    "                        mpqa_scores[subset]['negative_count'] += 1\r\n",
    "                    elif obj_score == max([pos_score, neg_score, obj_score]):\r\n",
    "                        mpqa_scores[subset]['objective_count'] += 1\r\n",
    "\r\n",
    "        mpqa_scores[subset]['average_positive_score'] = round(mpqa_scores[subset]['total_positive_score'] / mpqa_scores[subset]['positive_count'], 4)\r\n",
    "        mpqa_scores[subset]['average_negative_score'] = round(mpqa_scores[subset]['total_negative_score'] / mpqa_scores[subset]['negative_count'], 4)\r\n",
    "        mpqa_scores[subset]['average_objective_score'] = round(mpqa_scores[subset]['total_objective_score'] / mpqa_scores[subset]['objective_count'], 4)\r\n",
    "        mpqa_scores[subset]['std_dev_positive_score'] = round(stdev(pos_scores), 4)\r\n",
    "        mpqa_scores[subset]['std_dev_negative_score'] = round(stdev(neg_scores), 4)\r\n",
    "        mpqa_scores[subset]['std_dev_objective_score'] = round(stdev(obj_scores), 4)\r\n",
    "        mpqa_scores[subset]['overall_score'] = mpqa_scores[subset]['average_negative_score'] - mpqa_scores[subset]['average_positive_score']\r\n",
    "        mpqa_scores[subset]['positive_ratio'] = round(mpqa_scores[subset]['positive_count'] / mpqa_scores[subset]['total_num_words'], 4)\r\n",
    "        mpqa_scores[subset]['negative_ratio'] = round(mpqa_scores[subset]['negative_count'] / mpqa_scores[subset]['total_num_words'], 4)\r\n",
    "\r\n",
    "    return mpqa_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def socialsent_scores(texts, collection='smoke'):\r\n",
    "    socialsent_smoke_filepath = './lexicons/subreddits/electronic_cigarette.tsv'\r\n",
    "    socialsent_askscience_filepath = './lexicons/subreddits/askscience.tsv'\r\n",
    "    socialsent_science_filepath = './lexicons/subreddits/science.tsv'\r\n",
    "\r\n",
    "    if collection == 'smoke':\r\n",
    "        socialsent_df = pd.read_csv(socialsent_smoke_filepath, sep='\\t', names=['word', 'avg_sent_score', 'std_sent_score'])\r\n",
    "    elif collection == 'science':\r\n",
    "        socialsent_df = pd.read_csv(socialsent_science_filepath, sep='\\t', names=['word', 'avg_sent_score', 'std_sent_score'])\r\n",
    "\r\n",
    "    socialsent_scores = {}\r\n",
    "    word_list = {}\r\n",
    "    for subset in texts:\r\n",
    "        socialsent_scores[subset] = {\r\n",
    "            'total_positive_score': 0,\r\n",
    "            'total_negative_score': 0,\r\n",
    "            'total_objective_score': 0,\r\n",
    "            'average_positive_score': 0,\r\n",
    "            'std_dev_positive_score': 0,\r\n",
    "            'average_negative_score': 0,\r\n",
    "            'std_dev_negative_score': 0,\r\n",
    "            'average_objective_score': 0,\r\n",
    "            'std_dev_objective_score': 0,\r\n",
    "            'overall_score': 0,\r\n",
    "            'total_num_objective_posts': 0,\r\n",
    "            'total_num_words': 0,\r\n",
    "            'positive_count': 0,\r\n",
    "            'negative_count': 0,\r\n",
    "            'objective_count': 0,\r\n",
    "            'positive_ratio': 0,\r\n",
    "            'negative_ratio': 0\r\n",
    "        }\r\n",
    "        pos_scores = []\r\n",
    "        neg_scores = []\r\n",
    "        obj_scores = []\r\n",
    "        word_list[subset] = []\r\n",
    "        for line in texts[subset]:\r\n",
    "            for word in line:\r\n",
    "               if word in socialsent_df['word'].values:\r\n",
    "                    word_list[subset].append(word)\r\n",
    "                    socialsent_scores[subset]['total_num_words'] += 1\r\n",
    "                    pos_score = 0\r\n",
    "                    neg_score = 0\r\n",
    "                    obj_score = 0\r\n",
    "                    sent_score = socialsent_df.loc[socialsent_df['word'] == word]['avg_sent_score'].values[0]\r\n",
    "                    if abs(sent_score/10) < 0.1 :\r\n",
    "                        obj_score = 1\r\n",
    "                        socialsent_scores[subset]['total_objective_score'] += obj_score\r\n",
    "                        socialsent_scores[subset]['objective_count'] += 1\r\n",
    "                        obj_scores.append(obj_score)\r\n",
    "                    elif sent_score > 0:\r\n",
    "                        pos_score = sent_score/10\r\n",
    "                        socialsent_scores[subset]['total_positive_score'] += pos_score\r\n",
    "                        socialsent_scores[subset]['positive_count'] += 1\r\n",
    "                        pos_scores.append(pos_score)\r\n",
    "                    elif sent_score < 0:\r\n",
    "                        neg_score = abs(sent_score/10)\r\n",
    "                        socialsent_scores[subset]['total_negative_score'] += neg_score\r\n",
    "                        socialsent_scores[subset]['negative_count'] += 1\r\n",
    "                        neg_scores.append(neg_score)\r\n",
    "\r\n",
    "        socialsent_scores[subset]['average_positive_score'] = round(socialsent_scores[subset]['total_positive_score'] / socialsent_scores[subset]['positive_count'], 4)\r\n",
    "        socialsent_scores[subset]['average_negative_score'] = round(socialsent_scores[subset]['total_negative_score'] / socialsent_scores[subset]['negative_count'], 4)\r\n",
    "        socialsent_scores[subset]['average_objective_score'] = round(socialsent_scores[subset]['total_objective_score'] / socialsent_scores[subset]['objective_count'], 4)\r\n",
    "        socialsent_scores[subset]['std_dev_positive_score'] = round(stdev(pos_scores), 4)\r\n",
    "        socialsent_scores[subset]['std_dev_negative_score'] = round(stdev(neg_scores), 4)\r\n",
    "        socialsent_scores[subset]['std_dev_objective_score'] = round(stdev(obj_scores), 4)\r\n",
    "        socialsent_scores[subset]['overall_score'] = socialsent_scores[subset]['average_negative_score'] - socialsent_scores[subset]['average_positive_score']\r\n",
    "        socialsent_scores[subset]['positive_ratio'] = round(socialsent_scores[subset]['positive_count'] / socialsent_scores[subset]['total_num_words'], 4)\r\n",
    "        socialsent_scores[subset]['negative_ratio'] = round(socialsent_scores[subset]['negative_count'] / socialsent_scores[subset]['total_num_words'], 4)\r\n",
    "\r\n",
    "    return socialsent_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def build_word2vec_model_and_vocabulary(texts):\n",
    "    whole_text = []\n",
    "    vocabulary = {}\n",
    "    \n",
    "    for subset in texts:\n",
    "        vocabulary[subset] = []\n",
    "        for line in texts[subset]:\n",
    "            whole_text.append(line)\n",
    "            for word in line:\n",
    "                if word not in vocabulary[subset]:\n",
    "                    vocabulary[subset].append(word)\n",
    "    cores = os.cpu_count()-1\n",
    "    model = Word2Vec(sentences=whole_text,\n",
    "                     min_count=1,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     iter=40,\n",
    "                     workers=cores)\n",
    "    return model, vocabulary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def build_doc2vec_model(texts):\r\n",
    "    tagged_data = []\r\n",
    "    for subset in texts:\r\n",
    "        tagged_data += [TaggedDocument(words=word_tokenize(sentence), tags=[subset]) for i, sentence in enumerate(texts[subset])]\r\n",
    "    cores = os.cpu_count()-1\r\n",
    "    model = Doc2Vec(size=300, alpha=0.025, min_alpha=0.00025, min_count=1, workers=cores)\r\n",
    "    model.build_vocab(tagged_data)\r\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=40)\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def compute_word2vec_wmdistance(model, texts):\r\n",
    "    model.init_sims(replace=True)\r\n",
    "    wmdistances = {}\r\n",
    "    data = {}\r\n",
    "    for subset in texts:\r\n",
    "        data[subset] = []\r\n",
    "        for sentence in texts[subset]:\r\n",
    "            data[subset] += word_tokenize(sentence)\r\n",
    "    \r\n",
    "    for subset1 in data:\r\n",
    "        wmdistances[subset1] = {}\r\n",
    "        for subset2 in data:\r\n",
    "            wmdistances[subset1][subset2] = model.wv.wmdistance(data[subset1], data[subset2])\r\n",
    "    \r\n",
    "    return wmdistances"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def compute_word2vec_soft_cosine_similarity(model, texts, subsets1, subsets2):\r\n",
    "    termsim_index = WordEmbeddingSimilarityIndex(model.wv)\r\n",
    "    soft_cosine_similarities = {}\r\n",
    "    data = {}\r\n",
    "    common_data = []\r\n",
    "    for subset in texts:\r\n",
    "        data[subset] = []\r\n",
    "        for sentence in texts[subset]:\r\n",
    "            data[subset] += sentence\r\n",
    "        common_data.append(data[subset])\r\n",
    "    \r\n",
    "    dictionary = Dictionary(common_data)\r\n",
    "    bow_corpus = [dictionary.doc2bow(document) for document in common_data]\r\n",
    "    similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)  # construct similarity matrix\r\n",
    "    docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10)\r\n",
    "\r\n",
    "    subsets = list(texts.keys())\r\n",
    "    for subset1 in subsets1:\r\n",
    "        soft_cosine_similarities[subset1] = {}\r\n",
    "        # query = data[subset1]  # make a query\r\n",
    "        # sims = docsim_index[dictionary.doc2bow(query)]  # calculate similarity of query to each doc from bow_corpus\r\n",
    "        # print(sims)\r\n",
    "        for subset2 in subsets2:\r\n",
    "            soft_cosine_similarities[subset1][subset2] = similarity_matrix.inner_product(dictionary.doc2bow(data[subset1]), dictionary.doc2bow(data[subset2]), normalized=True)\r\n",
    "    return soft_cosine_similarities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def compute_doc2vec_soft_cosine_similarity(model, texts, subsets1, subsets2):\r\n",
    "    termsim_index = WordEmbeddingSimilarityIndex(model.wv)\r\n",
    "    soft_cosine_similarities = {}\r\n",
    "    data = {}\r\n",
    "    common_data = []\r\n",
    "    for subset in texts:\r\n",
    "        data[subset] = []\r\n",
    "        for sentence in texts[subset]:\r\n",
    "            data[subset] += word_tokenize(sentence)\r\n",
    "        common_data.append(data[subset])\r\n",
    "    \r\n",
    "    dictionary = Dictionary(common_data)\r\n",
    "    bow_corpus = [dictionary.doc2bow(document) for document in common_data]\r\n",
    "    similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)  # construct similarity matrix\r\n",
    "    docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=len(list(data.keys())))\r\n",
    "\r\n",
    "    subsets = list(texts.keys())\r\n",
    "    for subset1 in subsets1:\r\n",
    "        soft_cosine_similarities[subset1] = {}\r\n",
    "        # query = data[subset1]  # make a query\r\n",
    "        # sims = docsim_index[dictionary.doc2bow(query)]  # calculate similarity of query to each doc from bow_corpus\r\n",
    "        for subset2 in subsets2:\r\n",
    "            soft_cosine_similarities[subset1][subset2] = similarity_matrix.inner_product(dictionary.doc2bow(data[subset1]), dictionary.doc2bow(data[subset2]), normalized=True)\r\n",
    "    return soft_cosine_similarities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def compute_word2vec_cosine_similarity(model, vocabulary, subsets1, subsets2):\r\n",
    "    cosine_similarities = {}\r\n",
    "    vector_lists = {}\r\n",
    "    for subset in vocabulary:\r\n",
    "        for word in vocabulary[subset]:\r\n",
    "            if word in model.wv.vocab:\r\n",
    "                if subset not in vector_lists:\r\n",
    "                    vector_lists[subset] = []\r\n",
    "                vector_lists[subset] += list(model.wv[word])\r\n",
    "\r\n",
    "    for subset1 in subsets1:\r\n",
    "        cosine_similarities[subset1] = {}\r\n",
    "        for subset2 in subsets2:\r\n",
    "            original_vector_list1 = vector_lists[subset1].copy()\r\n",
    "            original_vector_list2 = vector_lists[subset2].copy()\r\n",
    "            if len(vector_lists[subset1]) < len(vector_lists[subset2]):\r\n",
    "                vector_lists[subset1] += [0] * (len(vector_lists[subset2]) - len(vector_lists[subset1]))\r\n",
    "            else:\r\n",
    "                vector_lists[subset2] += [0] * (len(vector_lists[subset1]) - len(vector_lists[subset2]))\r\n",
    "            cosine_similarities[subset1][subset2] = cosine_similarity([vector_lists[subset1]], [vector_lists[subset2]], dense_output=False)[0][0]\r\n",
    "            vector_lists[subset1] = original_vector_list1\r\n",
    "            vector_lists[subset2] = original_vector_list2\r\n",
    "    return cosine_similarities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def compute_word2vec_cosine_similarity_test(model, vocabulary, subsets1, subsets2):\r\n",
    "    cosine_similarities = {}\r\n",
    "    vector_lists = {}\r\n",
    "    for subset in vocabulary:\r\n",
    "        for word in vocabulary[subset]:\r\n",
    "            if word in model.wv.vocab:\r\n",
    "                if subset not in vector_lists:\r\n",
    "                    vector_lists[subset] = np.array([])\r\n",
    "                vector_lists[subset] = np.vstack([vector_lists[subset], model.wv[word]]) if vector_lists[subset].size else model.wv[word]\r\n",
    "\r\n",
    "    for subset1 in subsets1:\r\n",
    "        cosine_similarities[subset1] = {}\r\n",
    "        for subset2 in subsets2:\r\n",
    "            sum1 = np.zeros(len(vector_lists[subset1][0]))\r\n",
    "            sum2 = np.zeros(len(vector_lists[subset2][0]))\r\n",
    "            for vector in vector_lists[subset1]:\r\n",
    "                sum1 = np.add(sum1, vector)\r\n",
    "            for vector in vector_lists[subset2]:\r\n",
    "                sum2 = np.add(sum2, vector)\r\n",
    "            average1 = sum1/len(vector_lists[subset1])\r\n",
    "            average2 = sum2/len(vector_lists[subset2])\r\n",
    "            cosine_similarities[subset1][subset2] = cosine_similarity([average1], [average2], dense_output=False)[0][0]\r\n",
    "    return cosine_similarities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def compute_doc2vec_cosine_similarity(model, texts, subsets1, subsets2):\r\n",
    "    cosine_similarities = {}\r\n",
    "    data = {}\r\n",
    "    for subset in texts:\r\n",
    "        data[subset] = []\r\n",
    "        for sentence in texts[subset]:\r\n",
    "            data[subset] += word_tokenize(sentence)\r\n",
    "    for subset1 in subsets1:\r\n",
    "        cosine_similarities[subset1] = {}\r\n",
    "        inferred_vector = model.infer_vector(data[subset1])\r\n",
    "        most_similar_docs = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\r\n",
    "        # for similarity in model.docvecs.most_similar([model.docvecs[subset1]], topn=len(model.docvecs)):\r\n",
    "        #     subset2 = similarity[0]\r\n",
    "        #     cosine_similarity = similarity[1]\r\n",
    "        #     cosine_similarities[subset1][subset2] = cosine_similarity\r\n",
    "        for subset2 in subsets2:\r\n",
    "            # docvec1 = model.docvecs[subset1]\r\n",
    "            # docvec2 = model.docvecs[subset2]\r\n",
    "            # cosine_similarities[subset1][subset2] = cosine_similarity([docvec1], [docvec2])[0][0]\r\n",
    "            cosine_similarities[subset1][subset2] = next(doc[1] for doc in most_similar_docs if doc[0] == subset2)\r\n",
    "    return cosine_similarities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def write_sentiwordnet_scores(collection, senti_scores):\r\n",
    "    sentiwordnet_scores_directory = './results/sentiwordnet_scores/'\r\n",
    "    if not os.path.exists(sentiwordnet_scores_directory):\r\n",
    "        os.mkdir(sentiwordnet_scores_directory)\r\n",
    "    df = pd.DataFrame.from_dict(senti_scores, orient='index')\r\n",
    "    df.to_csv(sentiwordnet_scores_directory + collection + '_sentiwordnet_scores.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def write_mpqa_scores(collection, mpqa_scores):\r\n",
    "    mpqa_scores_directory = './results/mpqa_scores/'\r\n",
    "    if not os.path.exists(mpqa_scores_directory):\r\n",
    "        os.mkdir(mpqa_scores_directory)\r\n",
    "    df = pd.DataFrame.from_dict(mpqa_scores, orient='index')\r\n",
    "    df.to_csv(mpqa_scores_directory + collection + '_mpqa_scores.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def write_socialsent_scores(collection, socialsent_scores):\r\n",
    "    socialsent_scores_directory = './results/socialsent_scores/'\r\n",
    "    if not os.path.exists(socialsent_scores_directory):\r\n",
    "        os.mkdir(socialsent_scores_directory)\r\n",
    "    df = pd.DataFrame.from_dict(socialsent_scores, orient='index')\r\n",
    "    df.to_csv(socialsent_scores_directory + collection + '_socialsent_scores.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def write_similarity_measures(collection, similarities, similarity_type, filename_postfix, model_type='word2vec',):\r\n",
    "    similarity_directory = './results/'+ similarity_type + '/'\r\n",
    "    if not os.path.exists(similarity_directory):\r\n",
    "        os.mkdir(similarity_directory)\r\n",
    "    df = pd.DataFrame.from_dict(similarities, orient='index')\r\n",
    "    if model_type == 'word2vec':\r\n",
    "        df.to_csv(similarity_directory + collection + '_word2vec_' + filename_postfix + '.csv')\r\n",
    "    else:\r\n",
    "        df.to_csv(similarity_directory + collection + '_doc2vec_' + filename_postfix + '.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def write_unigrams_sentiwordnet_scores(collection, senti_scores):\r\n",
    "    sentiwordnet_scores_directory = './results/sentiwordnet_scores/'\r\n",
    "    if not os.path.exists(sentiwordnet_scores_directory):\r\n",
    "        os.mkdir(sentiwordnet_scores_directory)\r\n",
    "    index = [senti_scores[next(iter(senti_scores))][next(iter(senti_scores[next(iter(senti_scores))]))].keys(), list(senti_scores.keys())]\r\n",
    "    data = []\r\n",
    "    for subset1 in senti_scores:\r\n",
    "        data_slice = []\r\n",
    "        for key in senti_scores[subset1][next(iter(senti_scores[subset1].keys()))]:\r\n",
    "            for subset2 in senti_scores[subset1]:\r\n",
    "                data_slice.append(senti_scores[subset1][subset2][key])\r\n",
    "        data.append(data_slice)\r\n",
    "    mux = pd.MultiIndex.from_product(index)\r\n",
    "    df = pd.DataFrame(data, index=list(senti_scores.keys()), columns=mux)\r\n",
    "    df.to_csv(sentiwordnet_scores_directory + collection + '_unigrams_sentiwordnet_scores.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def write_unigrams_basic_analysis(collection, common_words):\r\n",
    "    unigrams_scores_directory = './results/unigrams/'\r\n",
    "    if not os.path.exists(unigrams_scores_directory):\r\n",
    "        os.mkdir(unigrams_scores_directory)\r\n",
    "    df = pd.DataFrame.from_dict(common_words, orient='index')\r\n",
    "    df.to_csv(unigrams_scores_directory + collection + '_unigrams_scores.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "i2b2_directory = './data/i2b2/smokers'\r\n",
    "reuters_directory = './data/reuters/processed_data'\r\n",
    "reddit_directory = './data/reddit/processed_data'\r\n",
    "\r\n",
    "i2b2_raw_directory = './data/i2b2/smokers_raw'\r\n",
    "reuters_raw_directory = './data/reuters/raw_data'\r\n",
    "reddit_raw_directory = './data/reddit/raw_data'\r\n",
    "\r\n",
    "results_directory = './results'\r\n",
    "if not os.path.exists(results_directory):\r\n",
    "    os.mkdir(results_directory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "i2b2_word2vec_texts, i2b2_doc2vec_texts = read_files(i2b2_directory)\r\n",
    "reuters_word2vec_texts, reuters_doc2vec_texts = read_files(reuters_directory)\r\n",
    "reddit_word2vec_texts, reddit_doc2vec_texts = read_files(reddit_directory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "i2b2_raw_texts = read_raw_files(i2b2_raw_directory)\r\n",
    "reuters_raw_texts = read_raw_files(reuters_raw_directory)\r\n",
    "reddit_raw_texts = read_raw_files(reddit_raw_directory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "i2b2_mpqa_scores = mpqa_scores(i2b2_word2vec_texts)\r\n",
    "reuters_mpqa_scores = mpqa_scores(reuters_word2vec_texts)\r\n",
    "reddit_mpqa_scores = mpqa_scores(reddit_word2vec_texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "write_mpqa_scores('i2b2', i2b2_mpqa_scores)\r\n",
    "write_mpqa_scores('reuters', reuters_mpqa_scores)\r\n",
    "write_mpqa_scores('reddit', reddit_mpqa_scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "i2b2_socialsent_scores = socialsent_scores(i2b2_raw_texts, 'smoke')\r\n",
    "reuters_socialsent_scores = socialsent_scores(reuters_raw_texts, 'science')\r\n",
    "reddit_socialsent_scores = socialsent_scores(reddit_raw_texts, 'smoke')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "write_socialsent_scores('i2b2', i2b2_socialsent_scores)\r\n",
    "write_socialsent_scores('reuters', reuters_socialsent_scores)\r\n",
    "write_socialsent_scores('reddit', reddit_socialsent_scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "i2b2_senti_scores, i2b2_tagged_vocabulary = sentiwordnet_scores(i2b2_word2vec_texts)\r\n",
    "reuters_senti_scores, reuters_tagged_vocabulary = sentiwordnet_scores(reuters_word2vec_texts)\r\n",
    "reddit_senti_scores, reddit_tagged_vocabulary = sentiwordnet_scores(reddit_word2vec_texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "write_sentiwordnet_scores('i2b2', i2b2_senti_scores)\r\n",
    "write_sentiwordnet_scores('reuters', reuters_senti_scores)\r\n",
    "write_sentiwordnet_scores('reddit', reddit_senti_scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "i2b2_unigrams = build_unigram_model(i2b2_word2vec_texts)\r\n",
    "reuters_unigrams = build_unigram_model(reuters_word2vec_texts)\r\n",
    "reddit_unigrams = build_unigram_model(reddit_word2vec_texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "i2b2_common_words, i2b2_common_tagged_words, i2b2_unigrams_senti_scores = evaluate_unigram_model(i2b2_unigrams, i2b2_tagged_vocabulary)\r\n",
    "reuters_common_words, reuters_common_tagged_words, reuters_unigrams_senti_scores = evaluate_unigram_model(reuters_unigrams, reuters_tagged_vocabulary)\r\n",
    "reddit_common_words, reddit_common_tagged_words, reddit_unigrams_senti_scores = evaluate_unigram_model(reddit_unigrams, reddit_tagged_vocabulary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from copy import deepcopy\r\n",
    "i2b2_num_of_common_words = deepcopy(i2b2_common_words)\r\n",
    "reuters_num_of_common_words = deepcopy(reuters_common_words)\r\n",
    "reddit_num_of_common_words = deepcopy(reddit_common_words)\r\n",
    "for dataset in [i2b2_num_of_common_words, reuters_num_of_common_words, reddit_num_of_common_words]:\r\n",
    "    for subset1 in dataset:\r\n",
    "        for subset2 in dataset[subset1]:\r\n",
    "            dataset[subset1][subset2] = len(dataset[subset1][subset2][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "write_unigrams_basic_analysis('i2b2', i2b2_num_of_common_words)\r\n",
    "write_unigrams_basic_analysis('reuters', reuters_num_of_common_words)\r\n",
    "write_unigrams_basic_analysis('reddit', reddit_num_of_common_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "write_unigrams_sentiwordnet_scores('i2b2', i2b2_unigrams_senti_scores)\r\n",
    "write_unigrams_sentiwordnet_scores('reuters', reuters_unigrams_senti_scores)\r\n",
    "write_unigrams_sentiwordnet_scores('reddit', reddit_unigrams_senti_scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "i2b2_word2vec_model, i2b2_vocabulary = build_word2vec_model_and_vocabulary(i2b2_word2vec_texts)\r\n",
    "reuters_word2vec_model, reuters_vocabulary = build_word2vec_model_and_vocabulary(reuters_word2vec_texts)\r\n",
    "reddit_word2vec_model, reddit_vocabulary = build_word2vec_model_and_vocabulary(reddit_word2vec_texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "i2b2_doc2vec_model = build_doc2vec_model(i2b2_doc2vec_texts)\r\n",
    "reuters_doc2vec_model = build_doc2vec_model(reuters_doc2vec_texts)\r\n",
    "reddit_doc2vec_model = build_doc2vec_model(reddit_doc2vec_texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "i2b2_word2vec_cosine_similarities = compute_word2vec_cosine_similarity(i2b2_word2vec_model, i2b2_vocabulary, list(i2b2_word2vec_texts.keys()), list(i2b2_word2vec_texts.keys()))\r\n",
    "reuters_word2vec_cosine_similarities = compute_word2vec_cosine_similarity(reuters_word2vec_model, reuters_vocabulary, list(reuters_word2vec_texts.keys()), list(reuters_word2vec_texts.keys()))\r\n",
    "reddit_word2vec_cosine_similarities = compute_word2vec_cosine_similarity(reddit_word2vec_model, reddit_vocabulary, list(reddit_word2vec_texts.keys()), list(reddit_word2vec_texts.keys()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "i2b2_doc2vec_cosine_similarities = compute_doc2vec_cosine_similarity(i2b2_doc2vec_model, i2b2_doc2vec_texts, list(i2b2_doc2vec_texts.keys()), list(i2b2_doc2vec_texts.keys()))\r\n",
    "reuters_doc2vec_cosine_similarities = compute_doc2vec_cosine_similarity(reuters_doc2vec_model, reuters_doc2vec_texts, list(reuters_doc2vec_texts.keys()), list(reuters_doc2vec_texts.keys()))\r\n",
    "reddit_doc2vec_cosine_similarities = compute_doc2vec_cosine_similarity(reddit_doc2vec_model, reddit_doc2vec_texts, list(reddit_doc2vec_texts.keys()), list(reddit_doc2vec_texts.keys()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "i2b2_word2vec_soft_cosine_similarities = compute_word2vec_soft_cosine_similarity(i2b2_word2vec_model, i2b2_word2vec_texts, list(i2b2_word2vec_texts.keys()), list(i2b2_word2vec_texts.keys()))\r\n",
    "reuters_word2vec_soft_cosine_similarities = compute_word2vec_soft_cosine_similarity(reuters_word2vec_model, reuters_word2vec_texts, list(reuters_word2vec_texts.keys()), list(reuters_word2vec_texts.keys()))\r\n",
    "reddit_word2vec_soft_cosine_similarities = compute_word2vec_soft_cosine_similarity(reddit_word2vec_model, reddit_word2vec_texts, list(reddit_word2vec_texts.keys()), list(reddit_word2vec_texts.keys()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "i2b2_doc2vec_soft_cosine_similarities = compute_doc2vec_soft_cosine_similarity(i2b2_doc2vec_model, i2b2_doc2vec_texts, list(i2b2_doc2vec_texts.keys()), list(i2b2_doc2vec_texts.keys()))\r\n",
    "reuters_doc2vec_soft_cosine_similarities = compute_doc2vec_soft_cosine_similarity(reuters_doc2vec_model, reuters_doc2vec_texts, list(reuters_doc2vec_texts.keys()), list(reuters_doc2vec_texts.keys()))\r\n",
    "reddit_doc2vec_soft_cosine_similarities = compute_doc2vec_soft_cosine_similarity(reddit_doc2vec_model, reddit_doc2vec_texts, list(reddit_doc2vec_texts.keys()), list(reddit_doc2vec_texts.keys()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "write_similarity_measures('i2b2', i2b2_word2vec_cosine_similarities, 'cosine_similarity', 'cosine_similarity')\r\n",
    "write_similarity_measures('reuters', reuters_word2vec_cosine_similarities, 'cosine_similarity', 'cosine_similarity')\r\n",
    "write_similarity_measures('reddit', reddit_word2vec_cosine_similarities, 'cosine_similarity', 'cosine_similarity')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "write_similarity_measures('i2b2', i2b2_doc2vec_cosine_similarities, 'cosine_similarity', 'cosine_similarity', 'doc2vec')\r\n",
    "write_similarity_measures('reuters', reuters_doc2vec_cosine_similarities, 'cosine_similarity', 'cosine_similarity', 'doc2vec')\r\n",
    "write_similarity_measures('reddit', reddit_doc2vec_cosine_similarities, 'cosine_similarity', 'cosine_similarity', 'doc2vec')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "write_similarity_measures('i2b2', i2b2_word2vec_soft_cosine_similarities, 'soft_cosine_similarity', 'soft_cosine_similarity')\r\n",
    "write_similarity_measures('reuters', reuters_word2vec_soft_cosine_similarities, 'soft_cosine_similarity', 'soft_cosine_similarity')\r\n",
    "write_similarity_measures('reddit', reddit_word2vec_soft_cosine_similarities, 'soft_cosine_similarity', 'soft_cosine_similarity')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "write_similarity_measures('i2b2', i2b2_doc2vec_soft_cosine_similarities, 'soft_cosine_similarity', 'soft_cosine_similarity', 'doc2vec')\r\n",
    "write_similarity_measures('reuters', reuters_doc2vec_soft_cosine_similarities, 'soft_cosine_similarity', 'soft_cosine_similarity', 'doc2vec')\r\n",
    "write_similarity_measures('reddit', reddit_doc2vec_soft_cosine_similarities, 'soft_cosine_similarity', 'soft_cosine_similarity', 'doc2vec')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "import random\r\n",
    "random.seed(10)\r\n",
    "all_synsets = swn.all_senti_synsets()\r\n",
    "pos_list = []\r\n",
    "neg_list = []\r\n",
    "for synset in all_synsets:\r\n",
    "    word = str(synset).replace('<', '').split('.')[0]\r\n",
    "    if synset.pos_score() == 1:\r\n",
    "        if word not in pos_list:\r\n",
    "            pos_list.append(word)\r\n",
    "    elif synset.neg_score() == 1:\r\n",
    "        if word not in neg_list:\r\n",
    "            neg_list.append(word)\r\n",
    "\r\n",
    "pos_list_words = random.sample(pos_list, 10)\r\n",
    "neg_list_words = random.sample(neg_list, 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "for texts in [i2b2_word2vec_texts, reuters_word2vec_texts, reddit_word2vec_texts]:\r\n",
    "    texts['pos_list'] = pos_list_words\r\n",
    "    texts['neg_list'] = neg_list_words\r\n",
    "\r\n",
    "for texts in [i2b2_doc2vec_texts, reuters_doc2vec_texts, reddit_doc2vec_texts]:\r\n",
    "    texts['pos_list'] = ' '.join(pos_list_words)\r\n",
    "    texts['neg_list'] = ' '.join(neg_list_words)\r\n",
    "\r\n",
    "for vocabulary in [i2b2_vocabulary, reuters_vocabulary, reddit_vocabulary]:\r\n",
    "    vocabulary['pos_list'] = pos_list_words\r\n",
    "    vocabulary['neg_list'] = neg_list_words\r\n",
    "\r\n",
    "for model in [i2b2_word2vec_model, reuters_word2vec_model, reddit_word2vec_model]:\r\n",
    "    model.train(pos_list_words, total_words=10, epochs=40)\r\n",
    "    model.train(neg_list_words, total_words=10, epochs=40)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "for model in [i2b2_doc2vec_model, reuters_doc2vec_model, reddit_doc2vec_model]:\r\n",
    "    model.train([TaggedDocument(words=pos_list_words, tags=['pos_list'])], total_examples=1, epochs=40)\r\n",
    "    model.train([TaggedDocument(words=neg_list_words, tags=['neg_list'])], total_examples=1, epochs=40)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "print(pos_list_words)\r\n",
    "print(neg_list_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['balmy', 'happiness', 'excellent', 'admirability', 'unsurpassable', 'good', 'bliss', 'praise', 'homologic', 'estimable']\n",
      "['dominated', 'abduction', 'scut_work', 'scrimy', 'unfortunate', 'deplorable', 'cad', 'mislead', 'disrespect', 'worst']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "i2b2_word2vec_modified_cosine_similarities = compute_word2vec_cosine_similarity(i2b2_word2vec_model, i2b2_vocabulary, ['pos_list', 'neg_list'], list(i2b2_word2vec_texts.keys())[:-2])\r\n",
    "reuters_word2vec_modified_cosine_similarities = compute_word2vec_cosine_similarity(reuters_word2vec_model, reuters_vocabulary, ['pos_list', 'neg_list'], list(reuters_word2vec_texts.keys())[:-2])\r\n",
    "reddit_word2vec_modified_cosine_similarities = compute_word2vec_cosine_similarity(reddit_word2vec_model, reddit_vocabulary, ['pos_list', 'neg_list'], list(reddit_word2vec_texts.keys())[:-2])\r\n",
    "\r\n",
    "i2b2_doc2vec_modified_cosine_similarities = compute_doc2vec_cosine_similarity(i2b2_doc2vec_model, i2b2_doc2vec_texts, ['pos_list', 'neg_list'], list(i2b2_doc2vec_texts.keys())[:-2])\r\n",
    "reuters_doc2vec_modified_cosine_similarities = compute_doc2vec_cosine_similarity(reuters_doc2vec_model, reuters_doc2vec_texts, ['pos_list', 'neg_list'], list(reuters_doc2vec_texts.keys())[:-2])\r\n",
    "reddit_doc2vec_modified_cosine_similarities = compute_doc2vec_cosine_similarity(reddit_doc2vec_model, reddit_doc2vec_texts, ['pos_list', 'neg_list'], list(reddit_doc2vec_texts.keys())[:-2])\r\n",
    "\r\n",
    "i2b2_word2vec_modified_soft_cosine_similarities = compute_word2vec_soft_cosine_similarity(i2b2_word2vec_model, i2b2_word2vec_texts, ['pos_list', 'neg_list'], list(i2b2_word2vec_texts.keys())[:-2])\r\n",
    "reuters_word2vec_modified_soft_cosine_similarities = compute_word2vec_soft_cosine_similarity(reuters_word2vec_model, reuters_word2vec_texts, ['pos_list', 'neg_list'], list(reuters_word2vec_texts.keys())[:-2])\r\n",
    "reddit_word2vec_modified_soft_cosine_similarities = compute_word2vec_soft_cosine_similarity(reddit_word2vec_model, reddit_word2vec_texts, ['pos_list', 'neg_list'], list(reddit_word2vec_texts.keys())[:-2])\r\n",
    "\r\n",
    "i2b2_doc2vec_modified_soft_cosine_similarities = compute_doc2vec_soft_cosine_similarity(i2b2_doc2vec_model, i2b2_doc2vec_texts, ['pos_list', 'neg_list'], list(i2b2_doc2vec_texts.keys())[:-2])\r\n",
    "reuters_doc2vec_modified_soft_cosine_similarities = compute_doc2vec_soft_cosine_similarity(reuters_doc2vec_model, reuters_doc2vec_texts, ['pos_list', 'neg_list'], list(reuters_doc2vec_texts.keys())[:-2])\r\n",
    "reddit_doc2vec_modified_soft_cosine_similarities = compute_doc2vec_soft_cosine_similarity(reddit_doc2vec_model, reddit_doc2vec_texts, ['pos_list', 'neg_list'], list(reddit_doc2vec_texts.keys())[:-2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "write_similarity_measures('i2b2', i2b2_word2vec_modified_cosine_similarities, 'cosine_similarity', 'pos_neg_list_cosine_similarity')\r\n",
    "write_similarity_measures('reuters', reuters_word2vec_modified_cosine_similarities, 'cosine_similarity', 'pos_neg_list_cosine_similarity')\r\n",
    "write_similarity_measures('reddit', reddit_word2vec_modified_cosine_similarities, 'cosine_similarity', 'pos_neg_list_cosine_similarity')\r\n",
    "\r\n",
    "write_similarity_measures('i2b2', i2b2_doc2vec_modified_cosine_similarities, 'cosine_similarity', 'pos_neg_list_cosine_similarity', 'doc2vec')\r\n",
    "write_similarity_measures('reuters', reuters_doc2vec_modified_cosine_similarities, 'cosine_similarity', 'pos_neg_list_cosine_similarity', 'doc2vec')\r\n",
    "write_similarity_measures('reddit', reddit_doc2vec_modified_cosine_similarities, 'cosine_similarity', 'pos_neg_list_cosine_similarity', 'doc2vec')\r\n",
    "\r\n",
    "write_similarity_measures('i2b2', i2b2_word2vec_modified_soft_cosine_similarities, 'soft_cosine_similarity', 'pos_neg_list_soft_cosine_similarity')\r\n",
    "write_similarity_measures('reuters', reuters_word2vec_modified_soft_cosine_similarities, 'soft_cosine_similarity', 'pos_neg_list_soft_cosine_similarity')\r\n",
    "write_similarity_measures('reddit', reddit_word2vec_modified_soft_cosine_similarities, 'soft_cosine_similarity', 'pos_neg_list_soft_cosine_similarity')\r\n",
    "\r\n",
    "write_similarity_measures('i2b2', i2b2_doc2vec_modified_soft_cosine_similarities, 'soft_cosine_similarity', 'pos_neg_list_soft_cosine_similarity', 'doc2vec')\r\n",
    "write_similarity_measures('reuters', reuters_doc2vec_modified_soft_cosine_similarities, 'soft_cosine_similarity', 'pos_neg_list_soft_cosine_similarity', 'doc2vec')\r\n",
    "write_similarity_measures('reddit', reddit_doc2vec_modified_soft_cosine_similarities, 'soft_cosine_similarity', 'pos_neg_list_soft_cosine_similarity', 'doc2vec')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ec4a4b1fed8cdcc39317a02ea85456e6a3ae5ccf8fcdd15a3382d60cec052de8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(directory):\n",
    "    texts = {}\n",
    "    for dir in os.listdir(directory):\n",
    "        texts[dir] = []\n",
    "        for root, _, files in os.walk(os.path.join(directory, dir)):\n",
    "            for file in files:\n",
    "                open_file = open(os.path.join(root, file), 'r')\n",
    "                text = open_file.read().split('\\n')\n",
    "                open_file.close()\n",
    "                texts[dir] += text\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiwordnet_scores(texts):\n",
    "    senti_scores = {}\n",
    "    for subset in texts:\n",
    "        senti_scores[subset] = {\n",
    "            'total_positive_score': 0,\n",
    "            'total_negative_score': 0,\n",
    "            'total_objective_score': 0,\n",
    "            'average_positive_score': 0,\n",
    "            'average_negative_score': 0,\n",
    "            'average_objective_score': 0,\n",
    "            'total_num_words': 0,\n",
    "            'positive_count': 0,\n",
    "            'negative_count': 0,\n",
    "            'objective_count': 0\n",
    "        }\n",
    "        for line in texts[subset]:\n",
    "            tagged = pos_tag(word_tokenize(line))\n",
    "            for tag in tagged:\n",
    "                synsets = swn.senti_synsets(tag[0], get_wordnet_pos(tag[1])) if get_wordnet_pos(tag[1]) != '' else []\n",
    "                synsets_list = list(synsets)\n",
    "                if len(synsets_list) > 0:\n",
    "                    senti_scores[subset]['total_num_words'] += 1\n",
    "                    synset = synsets_list[0]\n",
    "                    pos_score = synset.pos_score()\n",
    "                    neg_score = synset.neg_score()\n",
    "                    obj_score = synset.obj_score()\n",
    "                    senti_scores[subset]['total_positive_score'] += pos_score\n",
    "                    senti_scores[subset]['total_negative_score'] += neg_score\n",
    "                    senti_scores[subset]['total_objective_score'] += obj_score\n",
    "                    if pos_score > neg_score: \n",
    "                        senti_scores[subset]['positive_count'] += 1\n",
    "                    elif pos_score < neg_score:\n",
    "                        senti_scores[subset]['negative_count'] += 1\n",
    "                    else:\n",
    "                        senti_scores[subset]['objective_count'] += 1\n",
    "        senti_scores[subset]['average_positive_score'] = senti_scores[subset]['total_positive_score'] / senti_scores[subset]['total_num_words']\n",
    "        senti_scores[subset]['average_negative_score'] = senti_scores[subset]['total_negative_score'] / senti_scores[subset]['total_num_words']\n",
    "        senti_scores[subset]['average_objective_score'] = senti_scores[subset]['total_objective_score'] / senti_scores[subset]['total_num_words']\n",
    "    return senti_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2vec_model_and_vocabulary(texts):\n",
    "    whole_text = []\n",
    "    vocabulary = {}\n",
    "    model = Word2Vec(size=100, window=5, min_count=1, negative=10, workers=4)\n",
    "    for subset in texts:\n",
    "        vocabulary[subset] = []\n",
    "        for line in texts[subset]:\n",
    "            sentence = word_tokenize(line)\n",
    "            whole_text.append(sentence)\n",
    "            for word in sentence:\n",
    "                if word not in vocabulary[subset]:\n",
    "                    vocabulary[subset].append(word)\n",
    "    # model.build_vocab(sentence, update=True if len(model.wv.vocab) > 0 else False)\n",
    "    # model.train(whole_text, total_words=model.corpus_count, epochs=30)\n",
    "    model = Word2Vec(whole_text, size=100, window=5, min_count=1, negative=10, workers=4)\n",
    "    return model, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(model, vocabulary):\n",
    "    cosine_similarities = {}\n",
    "    # jaccard_similarities = {}\n",
    "    vector_lists = {}\n",
    "    for subset in vocabulary:\n",
    "        vector_lists[subset] = [model.wv[word] for word in vocabulary[subset]]\n",
    "\n",
    "    for subset1 in vector_lists:\n",
    "        cosine_similarities[subset1] = {}\n",
    "        # jaccard_similarities[subset1] = {}\n",
    "        for subset2 in vector_lists:\n",
    "            cosine_similarities[subset1][subset2] = sum(sum(cosine_similarity(vector_lists[subset1], vector_lists[subset2]))) / (len(vocabulary[subset1]) * len(vocabulary[subset2]))\n",
    "            # jaccard_similarities[subset1][subset2] = jaccard_score(vector_lists[subset1], vector_lists[subset2], average=None)\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sentiwordnet_scores(collection, senti_scores):\n",
    "    sentiwordnet_scores_directory = './results/sentiwordnet_scores/'\n",
    "    if not os.path.exists(sentiwordnet_scores_directory):\n",
    "        os.mkdir(sentiwordnet_scores_directory)\n",
    "    df = pd.DataFrame.from_dict(senti_scores, orient='index')\n",
    "    df.to_csv(sentiwordnet_scores_directory + collection + '_sentiwordnet_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_similarity_measures(collection, similarities):\n",
    "    consine_similarity_directory = './results/cosine_similarity/'\n",
    "    if not os.path.exists(consine_similarity_directory):\n",
    "        os.mkdir(consine_similarity_directory)\n",
    "    df = pd.DataFrame.from_dict(similarities, orient='index')\n",
    "    df.to_csv(consine_similarity_directory + collection + '_cosine_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2b2_directory = './data/i2b2/smokers'\n",
    "reuters_directory = './data/reuters/processed_data'\n",
    "reddit_directory = './data/reddit/processed_reddit_data.json'\n",
    "\n",
    "results_directory = './results'\n",
    "if not os.path.exists(results_directory):\n",
    "    os.mkdir(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2b2_texts = read_files(i2b2_directory)\n",
    "reuters_texts = read_files(reuters_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2b2_senti_scores = sentiwordnet_scores(i2b2_texts)\n",
    "reuters_senti_scores = sentiwordnet_scores(reuters_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sentiwordnet_scores('i2b2', i2b2_senti_scores)\n",
    "write_sentiwordnet_scores('reuters', reuters_senti_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2b2_model, i2b2_vocabulary = build_word2vec_model_and_vocabulary(i2b2_texts)\n",
    "reuters_model, reuters_vocabulary = build_word2vec_model_and_vocabulary(reuters_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2b2_cosine_similarities = compute_similarity(i2b2_model, i2b2_vocabulary)\n",
    "reuters_cosine_similarities = compute_similarity(reuters_model, reuters_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_similarity_measures('i2b2', i2b2_cosine_similarities)\n",
    "write_similarity_measures('reuters', reuters_cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'sci.crypt': {'sci.crypt': 0.8997260484860864, 'sci.electronics': 0.9077978524471398, 'sci.med': 0.9023777317886426, 'sci.space': 0.8907135498967576}, 'sci.electronics': {'sci.crypt': 0.9077978695981004, 'sci.electronics': 0.9168579432246009, 'sci.med': 0.9109681880108809, 'sci.space': 0.8985917812697923}, 'sci.med': {'sci.crypt': 0.9023777506377786, 'sci.electronics': 0.9109681974425523, 'sci.med': 0.9053187351112734, 'sci.space': 0.8932724838899536}, 'sci.space': {'sci.crypt': 0.8907135420077301, 'sci.electronics': 0.8985917496231003, 'sci.med': 0.8932724878596008, 'sci.space': 0.8818624898572911}}\n"
     ]
    }
   ],
   "source": [
    "print(reuters_cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'current smoker': {'current smoker': 0.7914213235085851, 'non-smoker': 0.7849269804711855, 'past smoker': 0.8039560093472362, 'smoker': 0.8046947242782889, 'unknown': 0.7373772263799789}, 'non-smoker': {'current smoker': 0.7849269700027701, 'non-smoker': 0.7785348474700405, 'past smoker': 0.7973867372449124, 'smoker': 0.7981133008404323, 'unknown': 0.7313544750768218}, 'past smoker': {'current smoker': 0.8039560206495121, 'non-smoker': 0.7973867445008836, 'past smoker': 0.8167993058761264, 'smoker': 0.817640259767548, 'unknown': 0.749039657832825}, 'smoker': {'current smoker': 0.8046947183185761, 'non-smoker': 0.7981132935480467, 'past smoker': 0.8176402318952525, 'smoker': 0.8188015597781435, 'unknown': 0.7496677159127629}, 'unknown': {'current smoker': 0.7373772390738277, 'non-smoker': 0.7313544899584205, 'past smoker': 0.749039651084836, 'smoker': 0.7496677173900353, 'unknown': 0.687085862376483}}\n"
     ]
    }
   ],
   "source": [
    "print(i2b2_cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}